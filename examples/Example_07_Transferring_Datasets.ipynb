{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Data\n",
    "\n",
    "This notebook shows you how to move data around.\n",
    "\n",
    "## Prep work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function  # For Python 2 compatibility\n",
    "import kosh\n",
    "import os\n",
    "import random\n",
    "\n",
    "def prep_stores(source_name=\"my_source_store.sql\", dest_name=\"my_dest_store.sql\", data_dir=\"my_data_dir\"):\n",
    "    \"\"\"\n",
    "    This creates two new stores and adds a dataset with 3 associated files to it to the first store\"\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        os.remove(source_name)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        os.remove(dest_name)\n",
    "    except:\n",
    "        pass\n",
    "    # Let's create a \"source\" and a \"destination\" store\n",
    "    source_store = kosh.connect(source_name, delete_all_contents=True)\n",
    "    dest_store = kosh.connect(dest_name, delete_all_contents=True)\n",
    "\n",
    "    # Let's create a dataset we'd like to transfer\n",
    "    dataset = source_store.create(name=\"a_dataset\", metadata={\"int_attr\":1, \"float_attr\":2., \"str_attr\": \"string\"})\n",
    "    \n",
    "    # let's create some files to associate\n",
    "    # first a directory\n",
    "    try:\n",
    "        os.makedirs(data_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "    filenames = [\"a.txt\", \"b.txt\", \"c.py\"]\n",
    "    filenames = [os.path.join(data_dir, f) for f in filenames]\n",
    "    \n",
    "    dataset.associate(filenames, \"test\")\n",
    "    for filename in filenames:\n",
    "        with open(filename, \"w\") as f:\n",
    "            print(\"some data\", file=f)\n",
    "            print(random.randint(0, 10000000), file=f)  # to ensure unique SHAs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring Datasets from one store to another w/o moving the data itself.\n",
    "\n",
    "In this case a simple python Python script will suffice, see [This Example](Example_Moving_Datasets.ipynb) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 5711bbd74ec7465ca105396d6e31fbaf\n",
      "\tname: a_dataset\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tcreator: cdoutrix\n",
      "\tfloat_attr: 2.0\n",
      "\tint_attr: 1\n",
      "\tname: a_dataset\n",
      "\tstr_attr: string\n",
      "--- Associated Data (3)---\n",
      "\tMime_type: test\n",
      "\t\tmy_data_dir/a.txt ( 2b7622d2728b4070afc0835ce7f4b724 )\n",
      "\t\tmy_data_dir/b.txt ( 21f0c796ed0742e39439e59a24d152bb )\n",
      "\t\tmy_data_dir/c.py ( 45911f91b38f4ab0ba0c7ad429409331 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's prepare the stores\n",
    "prep_stores()\n",
    "\n",
    "# Let's open our source store:\n",
    "my_store = kosh.connect(\"my_source_store.sql\")\n",
    "\n",
    "# Let's open our target store\n",
    "target_store = kosh.connect(\"my_dest_store.sql\")\n",
    "\n",
    "# Let's find the dataset(s) of interest in the source\n",
    "datasets = my_store.find(name=\"a_dataset\")\n",
    "\n",
    "# And let's transfer\n",
    "for dataset in datasets:\n",
    "    target_store.import_dataset(dataset.export())\n",
    "    \n",
    "# Voila! Let's check\n",
    "\n",
    "print(list(target_store.find(name=\"a_dataset\"))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data needs to be moved or copied.\n",
    "\n",
    "\n",
    "### On the same file system\n",
    "\n",
    "If you need to move some files simply use `kosh mv`\n",
    "\n",
    "Example: moving file.py to new_named_file.py\n",
    "\n",
    "```bash\n",
    "kosh mv --stores store1.sql --sources file.py --destination new_named_file.py\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "usage: kosh mv [-h] --stores STORES [--destination-stores DESTINATION_STORES] --sources SOURCES [SOURCES ...]\n",
    "               [--dataset_record_type DATASET_RECORD_TYPE] [--dataset_matching_attributes DATASET_MATCHING_ATTRIBUTES]\n",
    "               --destination DESTINATION [--version] [--merge_strategy {conservative,preserve,overwrite}] [--mk_dirs]\n",
    "```\n",
    "**OR within Python itself**\n",
    "```\n",
    "store.mv('file.py', 'new_named_file.py')\n",
    "```\n",
    "\n",
    "You can also copy files to another place and store\n",
    "\n",
    "```bash\n",
    "kosh cp --stores store1.sql --sources file.py --destination new_named_file.py\n",
    "```\n",
    "\n",
    "```\n",
    "usage: kosh cp [-h] --stores STORES [--destination-stores DESTINATION_STORES] --sources SOURCES [SOURCES ...]\n",
    "               [--dataset_record_type DATASET_RECORD_TYPE] [--dataset_matching_attributes DATASET_MATCHING_ATTRIBUTES]\n",
    "               --destination DESTINATION [--version] [--merge_strategy {conservative,preserve,overwrite}] [--mk_dirs]\n",
    "```\n",
    "**OR within Python itself**\n",
    "```\n",
    "store.cp('file.py', 'new_named_file.py')\n",
    "```\n",
    "\n",
    "Kosh should handle properly directories and patterns (*)\n",
    "\n",
    "### After the fact\n",
    "\n",
    "Ooops! You moved the files to a new place but forgot to do so via `kosh mv`\n",
    "\n",
    "Fear not! Kosh can probably help you fix your stores\n",
    "\n",
    "```\n",
    "usage: kosh reassociate --stores STORES --new_uris NEW_URIS [NEW_URIS ...] [--original_uris ORIGINAL_URIS [ORIGINAL_URIS ...]]\n",
    "            [--no_absolute_path]\n",
    "```\n",
    "\n",
    "\n",
    "#### Option 1: just point to the new files\n",
    "\n",
    "```bash\n",
    "kosh reassociate --stores store.sql --new_uris new_named_file.py\n",
    "```\n",
    "\n",
    "Kosh will compute the \"short sha\" on the target(s) and try to find a match.\n",
    "\n",
    "The *new_uris* can be a directory or pattern\n",
    "\n",
    "#### Option 2: Using the old name\n",
    "\n",
    "```bash\n",
    "kosh reassociate --stores store.sql --new_uris new_named_file.py --original_uris file.py\n",
    "```\n",
    "\n",
    "#### Option 3: I know the fast sha\n",
    "\n",
    "```bash\n",
    "kosh reassociate --stores store.sql --new_uris new_named_file.py --original_uris c6a15fa59ae2d070a88a6a96503543d4baeb8f381f247854ef04adb67f79d818\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving files across filesystem (remote host)\n",
    "\n",
    "Here we assume that we need to bring data from a remote machine\n",
    "\n",
    "Because Kosh will need to do a **LOT** of talking with the remote host\n",
    "it is preferable to setup an ssh agent so you do not need to type you password over and over\n",
    "\n",
    "Please [see this guide](https://docs.github.com/en/github/authenticating-to-github/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent) to setup yor keys and agent properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ·········\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(b'',\n",
       " b'Enter passphrase for /g/g19/cdoutrix/.ssh/id_rsa: Identity added: /g/g19/cdoutrix/.ssh/id_rsa (/g/g19/cdoutrix/.ssh/id_rsa)\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's ask for the password and setup ssh agent\n",
    "import getpass\n",
    "password = getpass.getpass()+\"\\n\"\n",
    "\n",
    "from subprocess import Popen, PIPE\n",
    "import shlex\n",
    "agent = Popen(\"ssh-agent\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "o,e = agent.communicate()\n",
    "for line in o.decode().split(\"\\n\"):\n",
    "    sp = line.split(\"=\")\n",
    "    if len(sp) > 1:\n",
    "        variable = sp[0]\n",
    "        value = sp[1].split(\";\")[0]\n",
    "        os.environ[variable] = value\n",
    "add = Popen(\"ssh-add\", stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "add.communicate(password.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our data\n",
    "\n",
    "prep_stores()\n",
    "my_store = kosh.connect(\"my_source_store.sql\")\n",
    "target_store = kosh.connect(\"my_dest_store.sql\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fake our \"remote host\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "user = getpass.getuser()\n",
    "hostname = socket.gethostname()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok all we need to do is to copy the data from the remote host to a new local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's cleanup first\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(\"my_new_data_dir\")\n",
    "except:\n",
    "    pass\n",
    "os.makedirs(\"my_new_data_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the command line to copy the data over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be executing:\n",
      "/g/g19/cdoutrix/miniconda3/envs/kosh/bin/kosh cp --stores my_source_store.sql --destination_stores my_dest_store.sql --sources cdoutrix@pascal83:/g/g19/cdoutrix/git/kosh/examples/my_data_dir --destination my_new_data_dir\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "cmd = \"{}/bin/kosh cp --stores my_source_store.sql --destination_stores my_dest_store.sql --sources {}@{}:{}/my_data_dir --destination my_new_data_dir\".format(sys.prefix, user, hostname, os.getcwd())\n",
    "print(\"We will be executing:\\n{}\".format(cmd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Popen(shlex.split(cmd), stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "o, e = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Now let's check our second store (on the remote) contains data\n",
    "remote_store = kosh.connect(\"my_dest_store.sql\")\n",
    "print(list(remote_store.find()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving files across disconnected filesystems\n",
    "\n",
    "Let's assume you have a LOT of data, you need to move it to another computer but you have a **VERY** slow connection to the other computer. \n",
    "\n",
    "Using scp/rsync will take months and you can't wait.\n",
    "\n",
    "Kosh solution at this point is to `tar` (or `htar`) your data on the original machine, manually transfer the data to the other machine (USB stick, DVD, etc...) and run tar again on the other end\n",
    "\n",
    "Kosh will look for the datasets referencing the files your tarring and add them to the tarball.\n",
    "\n",
    "When extracting Kosh will add these dataset (with the new local paths) to your destination store.\n",
    "\n",
    "The syntax is the same as your regular `tar`/`htar` (you can pass any command accepted by `tar`/`htar`) except you need to point to the kosh store and the tarball name **must be specified via -f**\n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "kosh tar cv --stores store1.sql store2.sql -f my_big_tar.tgz *.hdf5\n",
    "```\n",
    "\n",
    "Once one the destination machine you can do:\n",
    "\n",
    "```bash\n",
    "kosh tar cv --stores destination_store.sql -f my_big_tar.tgz\n",
    "```\n",
    "\n",
    "Your files are untarred and the dataset originally in store1 and store2 that pointed to these files are now in destination_store\n",
    "\n",
    "\n",
    "### Cleaning up store for dead files\n",
    "\n",
    "Sometimes files are gone either because someone else removed them or a disk failed, etc...\n",
    "\n",
    "Whatever the reason Kosh stores and datasets have capability to self clean.\n",
    "\n",
    "```bash\n",
    "kosh cleanup_files -s 'cmd_line.sql' # cleans up all non existing uri\n",
    "```\n",
    "\n",
    "Beware if you have URI pointing to non file sources they would be dissociated it is recommended to use a filter on datasets sources, e.g:\n",
    "\n",
    "```bash\n",
    "kosh cleanup_files -s 'cmd_line.sql' mime_type='hdf5' # cleans up all non existing uri pointing ot hdf5 mime_type sources\n",
    "```\n",
    "\n",
    "You can also accomplish the same thing from Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing /g/g19/cdoutrix/git/kosh/examples/my_data_dir/a.txt\n",
      "['/g/g19/cdoutrix/git/kosh/examples/my_data_dir/a.txt']\n",
      "KOSH DATASET\n",
      "\tid: edfd986619a640ea8d6e48be5f303a9b\n",
      "\tname: a_dataset\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tcreator: cdoutrix\n",
      "\tfloat_attr: 2.0\n",
      "\tint_attr: 1\n",
      "\tname: a_dataset\n",
      "\tstr_attr: string\n",
      "--- Associated Data (3)---\n",
      "\tMime_type: test\n",
      "\t\t/g/g19/cdoutrix/git/kosh/examples/my_data_dir/a.txt ( 622c1b84b9af49f18db6651019c9523d )\n",
      "\t\t/g/g19/cdoutrix/git/kosh/examples/my_data_dir/b.txt ( 66af741682fd4ea1959a24e9db9d70db )\n",
      "\t\t/g/g19/cdoutrix/git/kosh/examples/my_data_dir/c.py ( a7d5169831ea4f39bcf1b7f7972a3b18 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "\n",
      "['/g/g19/cdoutrix/git/kosh/examples/my_data_dir/a.txt']\n"
     ]
    }
   ],
   "source": [
    "dataset = list(my_store.find())[0]\n",
    "# Let's delete on of the files\n",
    "associated = next(dataset.find())\n",
    "print(\"Removing {}\".format(associated.uri))\n",
    "os.remove(associated.uri)\n",
    "print(dataset.cleanup_files(dry_run=True))\n",
    "print(dataset)\n",
    "print(dataset.cleanup_files())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
