{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Datasets Across Stores\n",
    "\n",
    "## The Basics\n",
    "\n",
    "Copying Kosh datasets from one store (`source_store`) to another store (`target_store`) is easy.\n",
    "\n",
    "In its simplest form one only needs to use the `import_dataset` function on the `target` store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KOSH DATASET\n",
       "\tid: 53587b1920b04aac9629ba5faec84b93\n",
       "\tname: example\n",
       "\tcreator: cdoutrix\n",
       "\n",
       "--- Attributes ---\n",
       "\tcreator: cdoutrix\n",
       "\tfoo: bar\n",
       "\tname: example\n",
       "--- Associated Data (0)---\n",
       "--- Ensembles (0)---\n",
       "\t[]\n",
       "--- Ensemble Attributes ---\n",
       "--- Alias Feature Dictionary ---"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kosh\n",
    "\n",
    "source_store = kosh.connect(\"source_store.sql\", delete_all_contents=True)\n",
    "target_store = kosh.connect(\"target_store.sql\", delete_all_contents=True)\n",
    "\n",
    "# Let's add a dataset to the source store\n",
    "dataset = source_store.create(name=\"example\")\n",
    "dataset.foo = \"bar\"\n",
    "\n",
    "# Let's import the dataset in our target store\n",
    "target_store.import_dataset(dataset)\n",
    "\n",
    "next(target_store.find(name=\"example\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging with existing datasets.\n",
    "\n",
    "When moving datasets from one store to another we need to consider the possibility that the receiving (`target_store`) store already contains one or many of the datasets imported from the incoming (`source_store`) store. In this case, Kosh will merge the imported dataset attributes and associated sources with the existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: c2ce0aa881dd429f824798d8041ede69\n",
      "\tname: example 2\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: bar\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: foo\n",
      "\tname: example 2\n",
      "--- Associated Data (1)---\n",
      "\tMime_type: notebook\n",
      "\t\t/g/g19/cdoutrix/git/kosh/examples/Example_Moving_Datasets.ipynb ( 3e83847d698d4a9bb476e12f4cd30255 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "KOSH DATASET\n",
      "\tid: 7bde01bc0b71428696f24b30eeba99d4\n",
      "\tname: example 2\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: foo\n",
      "\tfuzz: fuzzy\n",
      "\tname: example 2\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n"
     ]
    }
   ],
   "source": [
    "# Let's create a dataset with some attributes in the source store:\n",
    "d_source = source_store.create(name=\"example 2\", metadata={\"foo\":\"foo\", \"bar\":\"bar\"})\n",
    "# Let's associate some file\n",
    "d_source.associate(\"Example_Moving_Datasets.ipynb\",\"notebook\")\n",
    "print(d_source)\n",
    "\n",
    "# Let's create a similar dataset with the same name ('example') but different attributes in the target store:\n",
    "d_target = target_store.create(name=\"example 2\", metadata={\"foo\":\"foo\", \"fuzz\":\"fuzzy\"})\n",
    "print(d_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import `d_source` into the `target store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 7bde01bc0b71428696f24b30eeba99d4\n",
      "\tname: example 2\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: bar\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: foo\n",
      "\tfuzz: fuzzy\n",
      "\tname: example 2\n",
      "--- Associated Data (1)---\n",
      "\tMime_type: notebook\n",
      "\t\t/g/g19/cdoutrix/git/kosh/examples/Example_Moving_Datasets.ipynb ( 3e83847d698d4a9bb476e12f4cd30255 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n"
     ]
    }
   ],
   "source": [
    "target_store.import_dataset(d_source)\n",
    "print(d_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? Kosh ran a search on the target_store for dataset with the name attribute set to `example 2` it found our already existing dataset `d_target`. As a result Kosh merged the additional attributes and associated sources from `d_source` in `d_target`. As a result `d_target` contains all of its original attributes and associated sources plus the ones from `d_source`.\n",
    "***NOTE:*** Only the dataset in the **`target_store`** is altered, the dataset in the **`source_store`** is **NEVER** altered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this worked because Kosh only found 1 dataset in the `target_store` whose name matched the imported dataset.\n",
    "At times it is possible that multiple datasets would match. In this case Kosh would bail out.\n",
    "For example, let's create an additional dataset named \"example\" in each store (`dataset2` in `source_store` and `dataset3` in `target_store`).\n",
    "This means both `source_store` and `target_store` will now each have 2 datasets named `example`, but with different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original # of datasets named example in source store: 1\n",
      "Now, # of datasets named example in source store: 2\n",
      "Original # of datasets named example in target store: 1\n",
      "Now,  # of datasets named example in target store: 2\n"
     ]
    }
   ],
   "source": [
    "# Source store\n",
    "print(\"Original # of datasets named example in source store:\",len(list(source_store.find(name=\"example\"))))\n",
    "dataset2 = source_store.create(name=\"example\")\n",
    "print(\"Now, # of datasets named example in source store:\",len(list(source_store.find(name=\"example\"))))\n",
    "\n",
    "# Target store\n",
    "print(\"Original # of datasets named example in target store:\",len(list(target_store.find(name=\"example\"))))  # Only the dataset we imported earlier\n",
    "dataset3 = target_store.create(name=\"example\")\n",
    "print(\"Now,  # of datasets named example in target store:\",len(list(target_store.find(name=\"example\"))))  # The dataset we imported earlier and the one we just added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to import the `dataset2` from `source_store` into `target_store`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset criteria: {'name': 'example'} matches multiple (2) datasets in store target_store.sql, try changing 'match_attributes' when calling this function\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    target_store.import_dataset(dataset2)\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? \n",
    "\n",
    "When importing a dataset into a store, Kosh runs a search in the `target_store` store for all datasets with a matching `name` attribute.\n",
    "\n",
    "If multiple datasets are found with the same name, Kosh cannot uniquely determine which dataset to merge with.\n",
    "\n",
    "In our case `target_store` contains the dataset previously imported and `dataset3` which we just created. That means 2 datasets with the attribute `name` and value `example` are in the `target_Store` and Kosh cannot uniquely determine which it should merge with.\n",
    "\n",
    "In order to help Kosh we can use the `match_attributes` to help Kosh pinpoint our dataset. By default `match_attributes` is set to `[\"name\",]`\n",
    "\n",
    "Before going further, let's populate these newly created datasets with additional attributes.\n",
    "Some attribute (`bar`) will have the same value for each datasets, but others (`foo`, `foosome`) will have non-matching values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 5f51a3c9838d4ff5801fb6ae76449178\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar3\n",
      "\tfoosome: foo2\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "KOSH DATASET\n",
      "\tid: 53587b1920b04aac9629ba5faec84b93\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n"
     ]
    }
   ],
   "source": [
    "# Dataset in source_store\n",
    "dataset2.bar = \"foo\"\n",
    "dataset2.foo = \"bar2\"\n",
    "dataset2.foosome = \"foo1\"\n",
    "\n",
    "# Dataset in target store\n",
    "dataset3.bar = \"foo\"\n",
    "dataset3.foo = \"bar3\"\n",
    "dataset3.foosome = \"foo2\"\n",
    "\n",
    "# Let's print the dataset with `name` value of `example` in the target store\n",
    "for ds in target_store.find(name=\"example\"):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that asking Kosh to use `bar` as an additional `match_attribute` would let Kosh pinpoint a single dataset in the `target_store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in target_store.find(name=\"example\", bar=\"foo2\"):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to import dataset with attribute 'foo' value : bar2. But value for this attribute in target is 'bar3'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    target_store.import_dataset(dataset2, match_attributes=[\"name\", \"bar\"])\n",
    "except ValueError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened now?\n",
    "\n",
    "As expected, Kosh did find a unique dataset with the attributes `name` and `bar` matching our incoming dataset.\n",
    "So far, so good.\n",
    "\n",
    "Unfortunately the dataset in the `target_store` store shares a common attribute `foo` with our incoming dataset, and their values do not match.\n",
    "By default Kosh will bail out when conflicts arise.\n",
    "Otherwise the dataset in the **`target_store`** will be altered based on the values of the imported dataset (from `source_store`).\n",
    "***NOTE:*** Only the dataset in the **`target_store`** is altered, the dataset in the **`source_store`** is **NEVER** altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: bar2\n",
      "target: bar3\n"
     ]
    }
   ],
   "source": [
    "print(\"source:\", dataset2.foo)\n",
    "print(\"target:\", dataset3.foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately we can tell Kosh how to handle conflicts via the `merge_handler` attribute, which is set to `conservative` by default.\n",
    "\n",
    "Other options are `preserve` or `overwrite`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 0b9df67dd1b04f67a7d8042ea422d851\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar2\n",
      "\tfoosome: foo1\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attributes of interest on dataset2: bar2 foo1\n",
      "KOSH DATASET\n",
      "\tid: 5f51a3c9838d4ff5801fb6ae76449178\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar3\n",
      "\tfoosome: foo2\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attributes of interest on dataset3: bar3 foo2\n"
     ]
    }
   ],
   "source": [
    "target_store.import_dataset(dataset2, match_attributes=[\"name\", \"bar\"], merge_handler=\"preserve\")\n",
    "# Attributes are preserved (in the `target_store` only, the source dataset is never altered)\n",
    "print(dataset2)\n",
    "print(\"Attributes of interest on dataset2:\", dataset2.foo, dataset2.foosome)\n",
    "print(dataset3)\n",
    "print(\"Attributes of interest on dataset3:\", dataset3.foo, dataset3.foosome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 0b9df67dd1b04f67a7d8042ea422d851\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar2\n",
      "\tfoosome: foo1\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attributes of interest on dataset2: bar2 foo1\n",
      "KOSH DATASET\n",
      "\tid: 5f51a3c9838d4ff5801fb6ae76449178\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar2\n",
      "\tfoosome: foo1\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attributes of interest on dataset3: bar2 foo1\n"
     ]
    }
   ],
   "source": [
    "target_store.import_dataset(dataset2, match_attributes=[\"name\", \"bar\"], merge_handler=\"overwrite\")\n",
    "# Attributes are overwritten (in the `target_store`, the source dataset is never altered)\n",
    "print(dataset2)\n",
    "print(\"Attributes of interest on dataset2:\", dataset2.foo, dataset2.foosome)\n",
    "print(dataset3)\n",
    "print(\"Attributes of interest on dataset3:\", dataset3.foo, dataset3.foosome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kosh also let you pass your own custom handler function. This function will receive the `merge_handler_kargs` keyword arguments.\n",
    "\n",
    "The function declaration should be: `foo(store_dataset, imported_dataset_attributes_dict, section, **merge_handler_kargs)`\n",
    "\n",
    "Where:\n",
    " * `store_dataset` is the destination kosh dataset or its non-data section dictionary.\n",
    " * `imported_dataset_attributes_dict` is a dictionary of attributes/values of the dataset we're importing.\n",
    " * `section` is the section of the record being updated.\n",
    " * `merge_handler_kargs` is a dict of passed for this function.\n",
    "\n",
    "The function should return a dictionary of attributes/values that the target_dataset should have.\n",
    "\n",
    "Let's design a function that would overwrite some parameters but preserve others, based on the input keyword `overwrite_attributes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_handler(store_dataset, imported_dataset_dict, section, overwrite_attributes=[], **kargs):\n",
    "    # prepare the target dict\n",
    "    imported_attributes = imported_dataset_dict\n",
    "    target_attributes = {}\n",
    "    # We only care about the data section here\n",
    "    if section == \"data\":\n",
    "        store_attributes = store_dataset.list_attributes(dictionary=True)\n",
    "        target_attributes.update(imported_attributes)\n",
    "        target_attributes.update(store_attributes)\n",
    "        for attribute, value in imported_attributes.items():\n",
    "            if attribute in store_attributes:\n",
    "                if attribute in overwrite_attributes:\n",
    "                    target_attributes[attribute] = value\n",
    "    return target_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reset our dataset attributes and tell it to overwrite `foo` but not `foosome`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: 0b9df67dd1b04f67a7d8042ea422d851\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar2\n",
      "\tfoosome: foo1\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attribute of interest on dataset2: bar2 foo1\n",
      "KOSH DATASET\n",
      "\tid: 5f51a3c9838d4ff5801fb6ae76449178\n",
      "\tname: example\n",
      "\tcreator: cdoutrix\n",
      "\n",
      "--- Attributes ---\n",
      "\tbar: foo\n",
      "\tcreator: cdoutrix\n",
      "\tfoo: bar2\n",
      "\tfoosome: foo2\n",
      "\tname: example\n",
      "--- Associated Data (0)---\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n",
      "Attribute of interest on dataset3: bar2 foo2\n"
     ]
    }
   ],
   "source": [
    "dataset3.bar = \"foo\"\n",
    "dataset3.foo = \"bar3\"\n",
    "dataset3.foosome = \"foo2\"\n",
    "target_store.import_dataset(dataset2, match_attributes=[\"name\", \"bar\"], merge_handler=my_handler, merge_handler_kargs={\"overwrite_attributes\":[\"foo\",]})\n",
    "# Attribute foo is overwritten, foosome is preserved (in the target_store only, the source dataset is never altered)\n",
    "print(dataset2)\n",
    "print(\"Attribute of interest on dataset2:\", dataset2.foo, dataset2.foosome)\n",
    "print(dataset3)\n",
    "print(\"Attribute of interest on dataset3:\", dataset3.foo, dataset3.foosome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I just want to import everything and make copies\n",
    "\n",
    "If you do not wish to merge but simply import everything as copies, then set `match_attributes` to `[\"id\",]` as it is highly unlikely that 2 datasets created in different stores will end up with the same (randomly generated) id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 2 datasets ['5f51a3c9838d4ff5801fb6ae76449178', '53587b1920b04aac9629ba5faec84b93']\n",
      "importing: 2 datasets ['0b9df67dd1b04f67a7d8042ea422d851', '53587b1920b04aac9629ba5faec84b93']\n",
      "After: 3 datasets (One was already here)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/g19/cdoutrix/.conda/envs/kosh/lib/python3.9/site-packages/kosh/store.py:887: UserWarning: When searching by id use id_pool\n",
      "  warnings.warn(\"When searching by id use id_pool\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", len(list(target_store.find(name=\"example\"))), \"datasets\", list(target_store.find(name=\"example\", ids_only=True)))\n",
    "print(\"importing:\", len(list(source_store.find(name=\"example\"))), \"datasets\", list(source_store.find(name=\"example\", ids_only=True)))\n",
    "target_store.import_dataset(dataset2, match_attributes=[\"id\",])\n",
    "print(\"After:\", len(list(target_store.find(name=\"example\"))), \"datasets (One was already here)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I only want the metadata, not the curves\n",
    "\n",
    "Sometimes you only care about some sections of the sina record, for example only the `data` section and not the `curve_sets` section.\n",
    "\n",
    "Kosh can skip over specified sections you're not interested in, simply pass the section(s) to ignore via the `skip_sina_record_sections` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KOSH DATASET\n",
       "\tid: obj2\n",
       "\tname: ???\n",
       "\tcreator: ???\n",
       "\n",
       "--- Attributes ---\n",
       "\tparam1: 1\n",
       "\tparam2: 2\n",
       "\tparam3: 3.3\n",
       "--- Associated Data (1)---\n",
       "\tMime_type: image/png\n",
       "\t\tfoo.png ( obj2 )\n",
       "--- Ensembles (0)---\n",
       "\t[]\n",
       "--- Ensemble Attributes ---\n",
       "--- Alias Feature Dictionary ---"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_store = kosh.connect(\"temp.sql\", delete_all_contents=True)\n",
    "some_store.import_dataset(\"sina_curve_rec_2.json\", skip_sina_record_sections=[\"curve_sets\",])\n",
    "next(some_store.find()) # no curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I would like to apply filter when importing dataset\n",
    "\n",
    "Similarly to Sina, you can apply filtering functions while importing datasets, these function will be applied to each record you are importing in the order you passed them. Function are expected to accept a dataset as an input and return a dataset.\n",
    "\n",
    "Let's revisit the curve skipping concept from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: obj2\n",
      "\tname: ???\n",
      "\tcreator: ???\n",
      "\n",
      "--- Attributes ---\n",
      "\tparam1: 1\n",
      "\tparam2: 2\n",
      "\tparam3: 3.3\n",
      "--- Associated Data (1)---\n",
      "\tMime_type: image/png\n",
      "\t\tfoo.png ( obj2 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/g19/cdoutrix/.conda/envs/kosh/lib/python3.9/site-packages/kosh/store.py:268: UserWarning: Unknown user, you will be logged as anonymous user\n",
      "  warnings.warn(\"Unknown user, you will be logged as anonymous user\")\n"
     ]
    }
   ],
   "source": [
    "# First let's create a function that removes the curves\n",
    "def yank_curvesets(dataset):\n",
    "    cont = True\n",
    "    features = dataset.list_features()\n",
    "    for feature in features:\n",
    "        try:\n",
    "            dataset.remove_curve_or_curve_set(feature)\n",
    "        except Exception:\n",
    "            # was already removed or not a curve\n",
    "            pass\n",
    "    return dataset\n",
    "\n",
    "# Let's remove attribute presets\n",
    "def yank_presets(dataset):\n",
    "    if hasattr(dataset, \"presets\"):\n",
    "        delattr(dataset, \"presets\")\n",
    "    return dataset\n",
    "\n",
    "# now let's import the record into the store\n",
    "some_store = kosh.connect(\"temp.sql\", delete_all_contents=True)\n",
    "some_store.import_dataset(\"sina_curve_rec_2.json\",ingest_funcs=[yank_curvesets, yank_presets])\n",
    "print(next(some_store.find())) # No curve no attribute 'presets'\n",
    "del some_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I would like to apply filter I got from Sina\n",
    "\n",
    "Sina comes with some ingest function that based on Sina records. Kosh provides an operator to automatically convert datasets to record in input and record to datasets in output. This means you can use Sina ingest function in Kosh.\n",
    "\n",
    "Let's once revisit the curve skipping concept from above, with record-based functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSH DATASET\n",
      "\tid: obj2\n",
      "\tname: ???\n",
      "\tcreator: ???\n",
      "\n",
      "--- Attributes ---\n",
      "\tparam1: 1\n",
      "\tparam2: 2\n",
      "\tparam3: 3.3\n",
      "--- Associated Data (1)---\n",
      "\tMime_type: image/png\n",
      "\t\tfoo.png ( obj2 )\n",
      "--- Ensembles (0)---\n",
      "\t[]\n",
      "--- Ensemble Attributes ---\n",
      "--- Alias Feature Dictionary ---\n"
     ]
    }
   ],
   "source": [
    "# First let's create a function that removes the curves\n",
    "@kosh.utils.datasets_in_place_of_records\n",
    "def yank_curvesets(record):\n",
    "    record[\"curve_sets\"]={}\n",
    "    return record\n",
    "\n",
    "# Let's remove attribute presets\n",
    "@kosh.utils.datasets_in_place_of_records\n",
    "def yank_presets(record):\n",
    "    if hasattr(record[\"data\"], \"presets\"):\n",
    "        delattr(record[\"data\"], \"presets\")\n",
    "    return record\n",
    "\n",
    "# now let's import the record into the store\n",
    "some_store = kosh.connect(\"temp.sql\", delete_all_contents=True)\n",
    "some_store.import_dataset(\"sina_curve_rec_2.json\",ingest_funcs=[yank_curvesets, yank_presets])\n",
    "print(next(some_store.find())) # No curve no attribute 'presets'\n",
    "del some_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Associating stores vs importing stores.\n",
    "\n",
    "While importing a few datasets can be useful, there are cases where one might want to import an entire `sub_store` into a `central_store` , without worrying about merging.\n",
    "\n",
    "For one this operation can be time intensive. Furthermore if the `sub_store` keeps being edited, synchronizing between the two stores can become a real headache.\n",
    "\n",
    "A work around this is to open both stores and run your queries on each store. This too can become a burden if the number of stores to synchronize increases.\n",
    "\n",
    "Kosh can handle this under the hood for you. By simply associating another `sub_store` with your `central_store` you end up with an up-to-date union of both stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_in_central_store']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for name in [\"central_store.sql\", \"sub_store.sql\"]:\n",
    "    if os.path.exists(name):\n",
    "        os.remove(name)\n",
    "central_store = kosh.connect(\"central_store.sql\", delete_all_contents=True)\n",
    "sub_store = kosh.connect(\"sub_store.sql\", delete_all_contents=True)\n",
    "\n",
    "central_store.create(name = \"dataset_in_central_store\")\n",
    "sub_store.create(name = \"dataset_in_sub_store\")\n",
    "\n",
    "# only one dataset in central_store\n",
    "print([x.name for x in central_store.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's associate the `sub_store` with the `central_store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_in_central_store', 'dataset_in_sub_store']\n"
     ]
    }
   ],
   "source": [
    "central_store.associate(sub_store)\n",
    "# Two datasets in central_store\n",
    "print([x.name for x in central_store.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** By default store association is unilateral, e.g the `sub_store` stays untouched and will have no idea the `central_store` exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_in_sub_store']\n"
     ]
    }
   ],
   "source": [
    "print([x.name for x in sub_store.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At any time we can undo this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_in_central_store']\n"
     ]
    }
   ],
   "source": [
    "central_store.dissociate(sub_store)\n",
    "# only one dataset in central_store\n",
    "print([x.name for x in central_store.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now re-associate but this time making both stores associated with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_in_central_store', 'dataset_in_sub_store']\n",
      "['dataset_in_sub_store', 'dataset_in_central_store']\n"
     ]
    }
   ],
   "source": [
    "central_store.associate(sub_store, reciprocal=True)\n",
    "# Two datasets in central_store\n",
    "print([x.name for x in central_store.find()])\n",
    "# Two datasets in sub_store\n",
    "print([x.name for x in sub_store.find()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** It is worth mentioning that association will be further picked up if one of the stores is associated with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in 3rd store: []\n",
      "Stores associated with 3rd store (uris): ['sub_store.sql']\n",
      "Stores associated with sub_store: (uris) ['central_store.sql']\n",
      "Stores associated with central_store: (uris) ['sub_store.sql']\n",
      "Datasets in 3rd store after association with sub_store: ['dataset_in_sub_store', 'dataset_in_central_store']\n",
      "Dataset in 3rd store after dissociation ['dataset_in_sub_store']\n",
      "Stores associated with 3rd store [<kosh.store.KoshStore object at 0x1554bf4b9910>]\n",
      "Sub store retrieved from 3rd store via uri: <kosh.store.KoshStore object at 0x1554bf4b9910>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "third_store = kosh.connect(\"third_store.sql\", delete_all_contents=True)\n",
    "# No datasets in third_store\n",
    "print(\"Datasets in 3rd store:\", [x.name for x in third_store.find()])\n",
    "third_store.associate(sub_store)\n",
    "print(\"Stores associated with 3rd store (uris):\", list(central_store.get_associated_stores()))\n",
    "print(\"Stores associated with sub_store: (uris)\", list(sub_store.get_associated_stores()))\n",
    "print(\"Stores associated with central_store: (uris)\", list(third_store.get_associated_stores()))\n",
    "# Now we have 2 datasets in third_store\n",
    "print(\"Datasets in 3rd store after association with sub_store:\", [x.name for x in third_store.find()])\n",
    "\n",
    "# Dissociating sub_store from central_store:\n",
    "sub_store.dissociate(central_store, reciprocal=True)\n",
    "# Now we have 1 dataset in third_store\n",
    "print(\"Dataset in 3rd store after dissociation\", [x.name for x in third_store.find()])\n",
    "\n",
    "# Rather than the list of associated stores uris,\n",
    "# we can get the stores themselves:\n",
    "print(\"Stores associated with 3rd store\", list(third_store.get_associated_stores(uris=False)))\n",
    "\n",
    "# We can also get the associated store via its uri\n",
    "# this is important if you plan on doing more store association\n",
    "# as Kosh will consider two python stores to be different stores for this purpose.\n",
    "# To be safe, you can also re-open a store after association were changed\n",
    "print(\"Sub store retrieved from 3rd store via uri:\", third_store.get_associated_store(\"sub_store.sql\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kosh Environment",
   "language": "python",
   "name": "kosh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
